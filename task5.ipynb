{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44fe4d3-6022-4326-bb2c-5d14519fc1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1505a9c0-6e3b-4fcb-acf6-b9d4d65b1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Import Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e8f964-4a72-408a-87e8-5c3a8d99b484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 38000\n",
      "To be, or not to be, that is the question:\n",
      "Whether 'tis nobler in the mind to suffer\n",
      "The slings and arrows of outrageous fortune,\n",
      "Or to take arms against a sea of troubles\n",
      "And by opposing end them. To\n"
     ]
    }
   ],
   "source": [
    "#STEP 2: Load the Text File\n",
    "# Load text data\n",
    "# STEP 2: Load the Text File\n",
    "with open(r\"C:\\Users\\omkar\\OneDrive\\Desktop\\Handwriten_text_project\\Handwritten.txt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Repeat text to increase training data\n",
    "text = text * 50\n",
    "\n",
    "print(\"Total characters:\", len(text))\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a652ed47-864b-457f-a795-fb9f5b2222a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 45\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Character Mapping\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Get all unique characters\n",
    "unique_chars = sorted(set(text))\n",
    "char2idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "idx2char = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "print(f\"Total unique characters: {len(unique_chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0fcf9d-45b1-4fce-93f4-e5768ceec037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (37960, 40)\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Prepare Input Sequences\n",
    "\n",
    "\n",
    "sequence_length = 40\n",
    "input_sequences = []\n",
    "target_chars = []\n",
    "\n",
    "for i in range(len(text) - sequence_length):\n",
    "    input_seq = text[i:i+sequence_length]\n",
    "    target_char = text[i+sequence_length]\n",
    "    input_sequences.append([char2idx[c] for c in input_seq])\n",
    "    target_chars.append(char2idx[target_char])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_seq = np.array(input_sequences)\n",
    "y_seq = np.array(target_chars)\n",
    "\n",
    "print(f\"Input sequences shape: {X_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2f275a-99f3-4092-b960-ae97dc0782e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Normalize Inputs\n",
    "\n",
    "\n",
    "X_seq = X_seq / float(len(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44bf1a4-23e3-4ef5-a783-7550ab7b3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omkar\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,805</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m66,560\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m)                  │           \u001b[38;5;34m5,805\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,365</span> (282.68 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m72,365\u001b[0m (282.68 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,365</span> (282.68 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,365\u001b[0m (282.68 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 6: Build the LSTM Model\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_seq.shape[1], 1)))\n",
    "model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97f1e5e-31f1-416c-883a-104a35937b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 20ms/step - loss: 3.0551\n",
      "Epoch 2/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 2.4678\n",
      "Epoch 3/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 1.1316\n",
      "Epoch 4/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.3266\n",
      "Epoch 5/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0646\n",
      "Epoch 6/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.1170\n",
      "Epoch 7/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0114\n",
      "Epoch 8/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0054\n",
      "Epoch 9/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.2835\n",
      "Epoch 10/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0137\n",
      "Epoch 11/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.1885\n",
      "Epoch 12/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0144\n",
      "Epoch 13/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0051\n",
      "Epoch 14/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.1167\n",
      "Epoch 15/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0077\n",
      "Epoch 16/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0031\n",
      "Epoch 17/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0016\n",
      "Epoch 18/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 8.4417e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.1330\n",
      "Epoch 20/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0644\n",
      "Epoch 21/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.0632\n",
      "Epoch 22/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0109\n",
      "Epoch 23/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0034\n",
      "Epoch 24/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0017\n",
      "Epoch 25/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 9.1005e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 4.9410e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.1182\n",
      "Epoch 28/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0179\n",
      "Epoch 29/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0031\n",
      "Epoch 30/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0016\n",
      "Epoch 31/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 8.6904e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 4.9528e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0520\n",
      "Epoch 34/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0349\n",
      "Epoch 35/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0021\n",
      "Epoch 36/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 5.7502e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 3.2917e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 1.9170e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 1.1201e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 6.4993e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 3.7313e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 0.0536\n",
      "Epoch 44/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0020\n",
      "Epoch 45/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 8.3470e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 4.6442e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 2.7593e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 1.6701e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 1.0141e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 6.1438e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 3.6909e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.1971e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 0.1808\n",
      "Epoch 54/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0043\n",
      "Epoch 55/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - loss: 0.0017\n",
      "Epoch 56/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 8.3865e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 4.6453e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.6654e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0661\n",
      "Epoch 60/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - loss: 0.0015\n",
      "Epoch 61/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 7.3002e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 4.0479e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - loss: 2.3635e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 1.4106e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 8.4450e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 5.0334e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 21ms/step - loss: 0.0435\n",
      "Epoch 68/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 9.0557e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - loss: 4.6315e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.6476e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - loss: 1.5634e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 9.3478e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 5.6252e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - loss: 3.3753e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.0154e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - loss: 0.0371\n",
      "Epoch 77/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 0.0011\n",
      "Epoch 78/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 4.7633e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.6792e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 1.6116e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 1.0004e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 6.2791e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 3.9269e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.4281e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - loss: 1.4823e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 0.0365\n",
      "Epoch 87/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 8.7079e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 4.1098e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 2.3851e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 23ms/step - loss: 1.4584e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 9.0277e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 5.6178e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 23ms/step - loss: 3.4974e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - loss: 2.1709e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 1.3310e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 8.1243e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 34ms/step - loss: 4.9377e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 25ms/step - loss: 2.9866e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 22ms/step - loss: 1.7895e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m1187/1187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 59ms/step - loss: 1.0659e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x238c75c5160>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 7: Train the Model\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    X_seq.reshape((X_seq.shape[0], X_seq.shape[1], 1)),\n",
    "    y_seq,\n",
    "    epochs=100,        # Lower epochs if dataset is small\n",
    "    batch_size=32\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cce29da-bf6e-4a1f-8bde-19c24db4f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Generate Text using Trained LSTM Model\n",
    "\n",
    "def generate_handwritten_text(seed_text, num_chars=300, temperature=0.6):\n",
    "    \n",
    "    generated_text = seed_text\n",
    "    encoded_seed = [char2idx[c] for c in seed_text if c in char2idx]\n",
    "\n",
    "    for _ in range(num_chars):\n",
    "        x_input = np.zeros((1, sequence_length, 1))\n",
    "        \n",
    "        for t, char_index in enumerate(encoded_seed[-sequence_length:]):\n",
    "            x_input[0, t, 0] = char_index / float(len(unique_chars))\n",
    "        \n",
    "        predictions = model.predict(x_input, verbose=0)[0]\n",
    "        predictions = np.log(predictions + 1e-8) / temperature\n",
    "        probabilities = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        next_char_index = np.random.choice(len(unique_chars), p=probabilities)\n",
    "        next_char = idx2char[next_char_index]\n",
    "        \n",
    "        generated_text += next_char\n",
    "        encoded_seed.append(next_char_index)\n",
    "\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28504fac-8ddc-4843-97d3-aec2307cde91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be, rrRoo,,,,,,,nnReoRR ne be,eosgew\n",
      "bo   C t no  om sleep;\n",
      "To slrep, perchance to dream—ay, there's the rub:\n",
      "For in that sleep of death what dreams may come,\n",
      "When we have shuffled off this mortal coil,\n",
      "Must give us pause—there's the respect\n",
      "That makes calamity of so long life.\n",
      "\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy father and refuse thy name;\n",
      "Or, if thou wilt not, be but sworn my love,\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "start_string = \"To be, or not to be, \"\n",
    "print(generate_handwritten_text(start_string, 400, temperature=0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4376a61-be02-456e-a3f6-5a550dd6f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Report  : Task 5\n",
    "Handwritten Text Generation using Deep Learning\n",
    "\n",
    "CODSOFT Internship – Machine Learning Task\n",
    "\n",
    "1. Introduction\n",
    "\n",
    "Handwritten-style text generation is an important application of Natural Language Processing (NLP) and Deep\n",
    "Learning. It involves training a model to learn sequential patterns in text and generate new, human-like content \n",
    "character by character.\n",
    "\n",
    "This project focuses on building a character-level Recurrent Neural Network (RNN) using LSTM \n",
    "(Long Short-Term Memory) to generate handwritten-like text based on a given seed input.\n",
    "\n",
    "2. Objective\n",
    "The objectives of this project are:\n",
    "To preprocess raw text data for character-level modeling\n",
    "To map characters to numerical representations\n",
    "To train an LSTM-based deep learning model\n",
    "To generate meaningful handwritten-style text\n",
    "To understand the impact of sequence length and temperature on text generation\n",
    "\n",
    "3. Dataset Description\n",
    "The dataset is a text-based dataset containing literary handwritten-style content.\n",
    "Dataset Characteristics:\n",
    "Type: Plain text (.txt)\n",
    "Content: Literary text (dialogues, sentences, punctuation)\n",
    "Granularity: Character-level (letters, spaces, symbols)\n",
    "Each character is treated as an individual feature, and the task is to predict the next character in a sequence.\n",
    "\n",
    " 4. Data Preprocessing\n",
    "The preprocessing steps included:\n",
    "Reading the text file and converting it into lowercase text\n",
    "Extracting all unique characters from the dataset\n",
    "Creating character-to-index and index-to-character mappings\n",
    "Generating fixed-length character sequences (sequence length = 40)\n",
    "Normalizing the input data for better training performance\n",
    "These steps converted raw text into structured numerical data suitable for deep learning.\n",
    "\n",
    " 5. Exploratory Analysis\n",
    "\n",
    "The dataset contains alphabets, punctuation, and whitespace characters\n",
    "Character distribution helps the model learn writing patterns\n",
    "Sequential relationships between characters enable meaningful sentence generation\n",
    "Understanding character frequency and sequence length plays a crucial role in text generation quality.\n",
    "\n",
    " 6. Deep Learning Model\n",
    "The following model architecture was used:\n",
    " LSTM-Based Character Model\n",
    "Input Layer: Character sequences\n",
    "LSTM Layer: 128 hidden units\n",
    "Dense Output Layer with Softmax activation\n",
    "Loss Function: Sparse Categorical Crossentropy\n",
    "Optimizer: Adam\n",
    "The LSTM model effectively captures long-term dependencies in character sequences.\n",
    "\n",
    " 7. Model Training\n",
    "The model was trained for multiple epochs\n",
    "Batch size was set to ensure efficient learning\n",
    "Training focused on predicting the next character in a sequence\n",
    "The model gradually learned grammatical structure, spacing, and punctuation from the dataset.\n",
    "\n",
    "8. Text Generation\n",
    "After training, the model generates text using:\n",
    "A seed input string\n",
    "Temperature-based sampling to control randomness\n",
    "Character-by-character prediction\n",
    "Lower temperature produces more predictable text, while higher temperature increases creativity.\n",
    "\n",
    " 9. Results & Conclusion\n",
    "The trained LSTM model successfully generated coherent handwritten-style text:\n",
    "Generated text followed grammatical patterns\n",
    "Proper punctuation and spacing were maintained\n",
    "Output resembled the style of the training dataset\n",
    "Key Takeaways:\n",
    "Character-level LSTM models are effective for text generation\n",
    "Sequence length and temperature significantly affect output quality\n",
    "Deep learning can mimic human writing patterns when trained properly\n",
    "\n",
    "10. Future Improvements\n",
    "Train on larger and more diverse handwritten text datasets\n",
    "Experiment with Bidirectional LSTM or GRU\n",
    "Apply word-level or transformer-based models\n",
    "Fine-tune temperature sampling for better creativity\n",
    "Deploy the model as a web-based text generator\n",
    "Project Completed as Part of\n",
    "CODSOFT – Machine Learning Internship\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
